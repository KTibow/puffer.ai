<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>PufferLib Docs</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <main class="content">
        <nav class="nav-box">
            <ul>
                <li><a href="#post-1">Installation</a></li>
                <li><a href="#post-2">Training Demo</a></li>
                <li><a href="#post-3">Vectorization</a></li>
                <li><a href="#post-4">Emulation</a></li>
                <li><a href="#post-5">Policies</a></li>
                <li><a href="#post-6">Ocean Environments</a></li>
                <li><a href="#post-7">Third Party Environments</a></li>
            </ul>
        </nav>


        <p>PufferLib is a library for sane and simple reinforcement learning. Our key features are:</p>
                <ul>
                    <li><strong>Clean PuffeRL:</strong> Our training demo is a turbocharged version of CleanRL's PPO + LSTM with severalfold performance improvements and major quality of life upgrades. Train at 1M+ steps/second and easily run hyperparameter sweeps powered by CARBS on dozens of environments.
                    <li><strong>Vectorization:</strong> Synchronous and asynchronous parallel simulation at millions of steps per second. Our multiprocessing backend has native multiagent support and can be over 10x faster than Gymnasium's for some environments.</li>
                    <li><strong>Emulation:</strong> Compatibility tools that make working with complex environments a breeze. Your environment will still be in Gymnasium/PZ format, but it will use a subset of the API that is easier for most libraries to deal with. Even if you use nothing else from PufferLib, this layer is worth your time.</li>
                    <li><strong>Ocean:</strong> Our first-party suite of ultra performant environments written in C. They use our native PufferEnv API and each run 1M+ steps/second per CPU core. New environment contributions welcome!</li>
                </ul>

                <p>This tutorial contains everything you need to start doing RL 100x faster than most of the field. We also highly encourage you to read the PufferLib source. It's not like other RL libraries: PufferLib does what it does in a few thousand lines of very simple code. It is also much easier to get help when you're stuck. Our <a href="https://discord.gg/puffer" target="_blank">Discord</a> community is active and helpful.</p>
            </header>
        </article>


        <article id="post-1" class="blog-post">
            <header class="post-header">
                <h1>Installation</h1>
            </header>
            <h2>PufferTank (Recommended)</h2>
            <p><a href="https://github.com/pufferai/puffertank" target="_blank">PufferTank</a> is a prebuilt GPU Docker image with PufferLib and dependencies for all environments in the registry, including some that are slow and tricky to install.</p>
            <pre><code>git clone https://github.com/pufferai/puffertank
cd puffertank

# Get a shell. Will download the ~12GB image on first run.
bash docker.sh test</code></pre>
            <div class="info-box">
                <p>If you have not used containers before and just want everything to work, clone the repository and open it in VSCode. You will need to install the Dev Container plugin as well as Docker Desktop. VSCode will then detect the settings in .devcontainer and set up the container for you. Neovim (btw) is also included.</p>
            </div>

            <h2>Pip Installation (core only, no training demo)</h2>
            <p>PufferLib is available as a standard pip package:</p>
            <pre><code>pip install pufferlib

# Extras for tutorial
pip install pufferlib[cleanrl,atari]</code></pre>

            <h2>Source Installation</h2>
            <pre><code>git clone https://github.com/pufferai/pufferlib
cd pufferlib
pip install -e .

# Extras for tutorial
pip install -e .[cleanrl,atari]</code></pre>
        </article>

        <article id="post-2" class="blog-post">
            <header class="post-header">
                <h1>Training Demo</h1>
            </header>
 
            <p>PufferLib includes a few different training scripts. You will notice that they are in the base repository, not in the pip package. This is because they are based on CleanRL, which is not meant to be packaged. Rather, it is a white-box library that is meant to be copied and edited to suit your needs. Integrating PufferLib only requires changing a few lines:</p>
            <pre><code>python cleanrl_ppo_atari.py</code></pre>
            <p>This is already much faster than the original CleanRL code, but it is still several times slower than our main training demo. Some basics:</p>
            <pre><code>python demo.py
    --mode [train, eval, sweep-carbs]
    --env [env_name]
    --vec [serial, multiprocessing, native, ray]
    --track # Track on Weights and Biases. Set your username in demo.py
    --help # display a full list of options

# Get help
python demo.py --help

# Get help on a specific environment
python demo.py --help --env snake

# Train breakout with multiprocessing:
python demo.py --mode train --env breakout --vec multiprocessing

# Run a hyperparameter sweep on Ocean pong:
python demo.py --mode sweep-carbs --env pong --vec multiprocessing

# Train Ocean snake with native vectorization and wandb logs:
python demo.py --mode train --env snake --vec native --track

# Set train and env params from cli:
python demo.py --mode train --env snake --vec native --train.learning_rate 0.001 --env.num_snakes 512

# Eval a pretrained baseline model:
python demo.py --mode eval --env snake --vec native --baseline

# Eval a local checkpoint:
python demo.py --mode eval --env snake --vec native --eval-model-path your_model.pt</code></pre>
           <p>Compared to the original CleanRL code, our demo file (which loads clean_pufferl.py) supports asynchronous on-policy vectorization, better multi-agent training, a convenient cli dashboard, better WandB log and sweeps integration, and more. It's only around 1000 lines of code, most of which is logging.</p>
       </article>
       <article id="post-3" class="blog-post">
           <header class="post-header">
               <h1>Vectorization</h1>
           </header>
                <p>In RL, vectorization refers to the process of simulating multiple copies of an environment in parallel. Our Multiprocessing backend is fast -- much faster than Gymnasium's in most cases. Atari runs 50-60% faster synchronous and 5x faster async by our latest benchmark, and some environments like NetHack can be 10x faster even synchronous, with no API changes. Here's how to create a vectorized environment:</p>
                        <pre><code>

from pufferlib.environments import atari
env_creator = atari.env_creator('breakout')

import pufferlib.vector
vecenv = pufferlib.vector.make(
    env_creator, # A callable (class or function) that returns an env
    env_args: None, # A list of arguments to pass to each environment
    env_kwargs: None, # A list of dictionary keyword arguments to pass to each environment
    backend: Serial, # pufferlib.vector.[Serial|Multiprocessing|Native|Ray]
    num_envs: 1, # The total number of environments to create
    **kwargs # extra backend-specific options
)

# Make 4 copies of Breakout on the current process (Serial is the default)
vecenv = pufferlib.vector.make(env_creator, num_envs=4)

# Make 4 copies of Breakout, each on a separate process
puf_vec = pufferlib.vector.make(env_creator, num_envs=4,
    backend=pufferlib.vector.Multiprocessing)

# Make 4 copies of Breakout, 2 on each of 2 processes
puf_vec = pufferlib.vector.make(env_creator, num_envs=4,
    backend=pufferlib.vector.Multiprocessing, num_workers=2)

# Make 4 copies of Breakout, 2 on each of 2 processes,
# but only get two observations per step
puf_vec = pufferlib.vector.make(env_creator, num_envs=4,
    backend=pufferlib.vector.Multiprocessing, num_workers=2,
    batch_size=2)

# Make 1024 instances of Ocean breakout on the current process
from pufferlib.environments.ocean import breakout
puf_vec = pufferlib.vector.make(breakout.env_creator,
    backend=pufferlib.vector.Native,
    env_kwargs=[{'num_agents': 1024}],
)

# Notice that Native envs handle multiple instances internally.
# You can still multiprocess/async, but don't make multiple external
# copies per process.
vecenv = pufferlib.vector.make(env_creator, num_envs=2,
    backend=pufferlib.vector.Multiprocessing, batch_size=1)

# Synchronous API - reset/step
vecenv = pufferlib.vector.make(breakout.env_creator, num_envs=2,
    backend=pufferlib.vector.Multiprocessing)
start, steps, TIMEOUT = time.time(), 0, 3
while time.time() - start < TIMEOUT:
    puf_vec.step(puf_vec.action_space.sample())
    steps += 1

vecenv.close()
print('Puffer FPS:', steps*vecenv.num_envs/TIMEOUT)

# Async API - async_reset, send/recv
vecenv = pufferlib.vector.make(breakout.env_creator, num_envs=2,
    backend=pufferlib.vector.Multiprocessing, batch_size=1)
vecenv.async_reset()
start, steps, TIMEOUT = time.time(), 0, 3
while time.time() - start < TIMEOUT:
    vecenv.recv()
    vecenv.send(vecenv.action_space.sample())
    steps += 1

vecenv.close()
print('Puffer Async FPS:', steps*vecenv.num_envs/TIMEOUT)
</code></pre>
            <p>Our vectorization works on almost any Gymnasium/PettingZoo environment, not just the ones we have bound manually. All you have to do is wrap your environment with our Emulation layer, covered in the next section. PufferLib outperforms other vectorization implementations by implementing the following optimizations:</p>
            <ul>
                <li><strong>A Python implementation of EnvPool.</strong> Simulates more envs than are needed per batch and returns batches of observations as soon as they are ready. Requires using the async send/recv instead of the sync step API.</li>
                <li><strong>Multiple environments per worker.</strong> Important for fast environments.</li>
                <li><strong>Shared memory.</strong> Unlike Gymnasium's implementation, we use a single buffer that is shared across environments.</li>
                <li><strong>Shared flags.</strong> Workers busy-wait on an unlocked flag instead of signaling via pipes or queues. This virtually eliminates interprocess communication overhead. Pipes are used once per episode to communicate aggregated infos.</li>
                <li><strong>Zero-copy batching.</strong> Because we use a single buffer for shared memory, we can return observations from contiguous subsets of workers without ever copying observations. Only does not work for full-async mode.</li>
                <li><strong>Native multiagent support.</strong> It's not an extra wrapper or slow bolt-on feature. PufferLib treats single-agent and multi-agent environments the same. API differences are handled at the emulation level.</li>
            </ul>

</article>
    <article id="post-4" class="blog-post">
        <header class="post-header">
            <h1>Emulation</h1>
        </header>
        <p>Complex environments may have heirarchical observations and actions, variable numbers of agents, and other quirks that make them difficult to work with and incompatible with standard reinforcement learning libraries. PufferLib's emulation layer makes every environment look like it has flat observations/actions and a constant number of agents. Here's how it works with NetHack and Neural MMO, two notoriously complex environments.</p>

        <pre><code>import pufferlib.emulation
import pufferlib.wrappers

import nle, nmmo

def nmmo_creator():
    env = nmmo.Env()
    env = pufferlib.wrappers.PettingZooTruncatedWrapper(env)
    return pufferlib.emulation.PettingZooPufferEnv(env=env)

def nethack_creator():
    return pufferlib.emulation.GymnasiumPufferEnv(env_creator=nle.env.NLE)

env = nethack_creator()
obs, _ = env.reset()
print('Emulate observation shape:', obs.shape)

# Unflatten to original shape
structured_obs = obs.view(env.obs_dtype)
print('Original observation shape:', structured_obs.shape)
</code></pre>
        <p>The wrappers give you back a Gymnasium/PettingZoo compliant environment. There is no loss of generality and no change to the underlying environment. You can wrap environments by class, creator function, or object, with or without additional arguments. These wrappers enable us to make some optimizations to vectorization code that would be difficult to implement otherwise.</p>
    </article>
    <article id="post-5" class="blog-post">
        <header class="post-header">
            <h1>Policies</h1>
        </header>
        <p>You don't want another Policy API so we don't have one. Just write normal PyTorch code. We do provide a wrapper for CleanRL compatibility.</p>
<pre><code>
import torch
from torch import nn
import numpy as np

import pufferlib.frameworks.cleanrl

class Policy(nn.Module):
  def __init__(self, env):
      super().__init__()
      self.encoder = nn.Linear(np.prod(
          envs.single_observation_space.shape), 128)
      self.decoders = nn.ModuleList([nn.Linear(128, n)
          for n in envs.single_action_space.nvec])
      self.value_head = nn.Linear(128, 1)

  def forward(self, env_outputs):
      env_outputs = env_outputs.reshape(env_outputs.shape[0], -1)
      hidden = self.encoder(env_outputs)
      actions = [dec(hidden) for dec in self.decoders]
      value = self.value_head(hidden)
      return actions, value

obs = torch.Tensor(obs)
policy = Policy(envs.driver_env)
cleanrl_policy = pufferlib.frameworks.cleanrl.Policy(policy)
actions = cleanrl_policy.get_action_and_value(obs)[0].numpy()
obs, rewards, terminals, truncateds, infos = envs.step(actions)
envs.close()</code></pre>

        <p>Optionally, you can class break the forward pass into an encode and decode step, which allows us to handle recurrance for you. So far, the code above is fully general and does not rely on PufferLib support for specific environments. For convenience, we also provide environment hooks with standard wrappers and baseline models. Here's a complete example.</p>
<pre><code>
import torch

import pufferlib.models
import pufferlib.vector
import pufferlib.frameworks.cleanrl
import pufferlib.environments.nmmo

make_env = pufferlib.environments.nmmo.env_creator()
envs = pufferlib.vector.make(make_env, backend=backend, num_envs=4)

policy = pufferlib.environments.nmmo.Policy(envs.driver_env)
cleanrl_policy = pufferlib.frameworks.cleanrl.Policy(policy)

env_outputs = envs.reset()[0]
obs = torch.from_numpy(env_outputs)
actions = cleanrl_policy.get_action_and_value(obs)[0].numpy()
next_obs, rewards, terminals, truncateds, infos = envs.step(actions)
envs.close()</code></pre>

        <p>It's that simple -- almost. Remember the unflatten operation in Emulation? We provide a PyTorch version of that which you'll want to use in any environment with structured observations.</p>

        <pre><code>
dtype = pufferlib.pytorch.nativize_dtype(envs.driver_env.emulated)
env_outputs = pufferlib.pytorch.nativize_tensor(obs, dtype)
print('Packed tensor:', obs.shape)
print('Unpacked:', env_outputs.keys())</code></pre>

    </article>
    <article id="post-6" class="blog-post">
        <header class="post-header">
            <h1>Puffer Ocean: First Party Envs & API</h1>
        </header>
        <p>Gymnasium and PettingZoo are great, but they cause fundamental performance limitations that cap enviroments to far below 1M steps/second:</p>
        <ul>
            <li>They are single-environment formats, not vector formats. This means that the loop over environments is done in Python.</li>
            <li>They require you to directly return observations, rather than writing into a shared buffer. This incurs redundant copy operations.</li>
            <li>There is a different API for single-agent and multi-agent environments. This causes compatibility issues, and the multi-agent API has a lot of Python overhead.</li>
        </ul>
        <p>PufferLib ships with <a href="ocean.html">Ocean</a>, our first-party suite of high-performance environments. They are written with our native PufferEnv API, which eliminates traditional bottlenecks. For debugging new code, Ocean also includes specially designed sanity environments that train in seconds and will let you catch 90% of implementation bugs.</p>

        <p>PufferEnv is a vector format that easily scales to several million steps per second. It is very similar to Gymnasium's VectorEnv, but with some elements preserved from PettingZoo to better support multi-agent envs without inconveniencing single-agent use. The Python binding is included below. The full source is available in ocean/squared.</p>
        <pre><code>
'''A simple sample environment. Use this as a template for your own envs.'''

import gymnasium
import numpy as np

import pufferlib
from pufferlib.environments.ocean.squared.cy_squared import CySquared


class Squared(pufferlib.PufferEnv):
    def __init__(self, num_envs=1, render_mode=None, size=11, buf=None):
        self.single_observation_space = gymnasium.spaces.Box(low=0, high=1,
            shape=(size*size,), dtype=np.uint8)
        self.single_action_space = gymnasium.spaces.Discrete(5)
        self.render_mode = render_mode
        self.num_agents = num_envs

        super().__init__(buf)
        self.c_envs = CySquared(self.observations, self.actions,
            self.rewards, self.terminals, num_envs, size)
 
    def reset(self, seed=None):
        self.c_envs.reset()
        return self.observations, []

    def step(self, actions):
        self.actions[:] = actions
        self.c_envs.step()

        episode_returns = self.rewards[self.terminals]

        info = []
        if len(episode_returns) > 0:
            info = [{
                'reward': np.mean(episode_returns),
            }]

        return (self.observations, self.rewards,
            self.terminals, self.truncations, info)

    def render(self):
        self.c_envs.render()

    def close(self):
        self.c_envs.close()
</code></pre>
            <p>The key feature here is the allocation of data buffers for observations, rewards, etc. When multiprocessing, PufferLib will place these into a single shared memory tensor, so your data will be available on the main process immediately. In other words, your environment computes observations directly into shared memory with no redundant copies or slow Python glue. The base class is less than 100 lines. We suggest reading it just to see that there is no magic.</p>

            <p>The logic for this environment is imported. It is written in C with an intermediate binding in Cython. This is not part of the API. You can implement environment logic in Python or any other language you like. The only hard requirement is that you write data into the provided self.observations, self.actions, self.rewards, self.terminals, and self.truncations buffers. Additionally, there are two soft requirements that we plan to revisit. First, PufferEnvs must handle multiple environment copies internally. This one is easy to eliminate, but you probably don't want Python running this loop anyways. Second, your environment must handle resets internally. This one is just easier for now, and we will revisit if it becomes an issue.</p>

            <p>A key goal of PufferLib is to give you the maximum performance based on the amount of engineering effort you are willing to put into your environment. Here is a full list of options, from fastest to slowest:</p>
                <ul>
                <li><strong>Native PufferEnv in C:</strong> All env logic in C. Carefully written C++ or any other language with a good binding to Python is fine too, but we find C the easiest. All of our new Ocean envs are written this way. One nice benefit is that C compiles to WASM, so it is easy to run web demos.</li>
                <li><strong>Native PufferEnv in Cython:</strong> All env logic in Cython. This can be as fast as C but requires careful optimization. We provide this option because it is easy for researchers familiar with Python, and we use Cython as a binding layer for C envs anyways, so it is easy to port later.</li>
                <li><strong>Native PufferEnv in Python:</strong> Uses our native API but keeps all logic in Python. Because of the way we manage observation memory, this can still be much faster than Gymnasium/PettingZoo</li>
                <li><strong>Gymnasium/PettingZoo:</strong>: Write a standard pre-pufferlib environment and use our Gymnasium/PettingZoo emulation layer for fast parallel simulation.</li>
                <li><strong>Not using PufferLib:</strong> A return to the dark ages.</li>
            </ul>
    </article>


            <article id="post-7" class="blog-post">
                <header class="post-header">
                    <h1>Third Party Environments</h1>
                </header>

                <p>You can use any well-behaved environment with PufferLib via a 1-line wrapper. These are just the environments we have gotten around to binding manually. A lot of environments are not well behaved and subtly deviate from the Gymnasium/PettingZoo API, or they use obscure features that most libraries are not designed to handle. Our bindings just help clean this up a bit. Plus, we add any tricky system package dependencies to PufferTank. Feel free to PR new bindings or fixes for existing ones!</p>


            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/openai/gym" target="_blank">
                        <img src="https://img.shields.io/github/stars/openai/gym?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star OpenAI Gym" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/openai/gym">OpenAI Gym</a> is the standard API for single-agent reinforcement learning environments. It also contains some built-in environments. We include <a href="https://www.gymlibrary.dev/environments/box2d/">Box2D</a> in our registry.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/PWhiddy/PokemonRedExperiments" target="_blank">
                        <img src="https://img.shields.io/github/stars/PWhiddy/PokemonRedExperiments?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Pokemon Red" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/PWhiddy/PokemonRedExperiments">Pokemon Red</a> is one of the original Pokemon games for gameboy. This project uses the game as an environment for reinforcement learning. We are actively supporting development on this one!</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Farama-Foundation/PettingZoo" target="_blank">
                        <img src="https://img.shields.io/github/stars/Farama-Foundation/PettingZoo?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star PettingZoo" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://pettingzoo.farama.org">PettingZoo</a> is the standard API for multi-agent reinforcement learning environments. It also contains some built-in environments. We include <a href="https://pettingzoo.farama.org/environments/butterfly/">Butterfly</a> in our registry.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Farama-Foundation/Arcade-Learning-Environment" target="_blank">
                        <img src="https://img.shields.io/github/stars/Farama-Foundation/Arcade-Learning-Environment?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Arcade Learning Environment" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/Farama-Foundation/Arcade-Learning-Environment">Arcade Learning Environment</a> provides a Gym interface for classic Atari games. This is the most popular benchmark for reinforcement learning algorithms.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Farama-Foundation/Minigrid" target="_blank">
                        <img src="https://img.shields.io/github/stars/Farama-Foundation/Minigrid?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Minigrid" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/Farama-Foundation/Minigrid">Minigrid</a> is a 2D grid-world environment engine and a collection of builtin environments. The target is flexible and computationally efficient RL research.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/geek-ai/MAgent" target="_blank">
                        <img src="https://img.shields.io/github/stars/geek-ai/MAgent?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star MAgent" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/geek-ai/MAgent/blob/master/doc/get_started.md">MAgent</a> is a platform for large-scale agent simulation.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/neuralmmo/environment" target="_blank">
                        <img src="https://img.shields.io/github/stars/openai/neural-mmo?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Neural MMO" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://neuralmmo.github.io">Neural MMO</a> is a massively multiagent environment for reinforcement learning. It combines large agent populations with high per-agent complexity and is the most actively maintained (by me) project on this list.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/openai/procgen" target="_blank">
                        <img src="https://img.shields.io/github/stars/openai/procgen?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Procgen" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/openai/procgen">Procgen</a> is a suite of arcade games for reinforcement learning with procedurally generated levels. It is one of the most computationally efficient environments on this list.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/facebookresearch/nle" target="_blank">
                        <img src="https://img.shields.io/github/stars/facebookresearch/nle?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star NLE" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/facebookresearch/nle">Nethack Learning Environment</a> is a port of the classic game NetHack to the Gym API. It combines extreme complexity with high simulation efficiency.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/facebookresearch/minihack" target="_blank">
                        <img src="https://img.shields.io/github/stars/facebookresearch/minihack?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star MiniHack" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/facebookresearch/nle">MiniHack Learning Environment</a> is a stripped down version of NetHack with support for level editing and custom procedural generation.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/danijar/crafter" target="_blank">
                        <img src="https://img.shields.io/github/stars/danijar/crafter?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Crafter" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/danijar/crafter">Crafter</a> is a top-down 2D Minecraft clone for RL research. It provides pixel observations and relatively long time horizons.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Emerge-Lab/gpudrive" target="_blank">
                        <img src="https://img.shields.io/github/stars/Emerge-Lab/gpudrive?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star GPUDrive" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/Emerge-Lab/gpudrive">GPUDrive</a> GPUDrive is a GPU-accelerated, multi-agent driving simulator that runs at 1 million FPS.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Bam4d/Griddly" target="_blank">
                        <img src="https://img.shields.io/github/stars/Bam4d/Griddly?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Griddly" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://griddly.readthedocs.io/en/latest/">Griddly</a> is an extremely optimized platform for building reinforcement learning environments. It also includes a large suite of built-in environments.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Farama-Foundation/MicroRTS-Py" target="_blank">
                        <img src="https://img.shields.io/github/stars/Farama-Foundation/MicroRTS-Py?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star MicroRTS-Py" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/Farama-Foundation/MicroRTS-Py">Gym MicroRTS</a> is a real time strategy engine for reinforcement learning research. The Java configuration is a bit finicky -- we're still debugging this.</p>
                </div>
            </div>
            </article>

        </div>
    </main>
    <script src="header.js"></script>
    <link rel="stylesheet" href="highlight.css">
    <script src="highlight.js"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
