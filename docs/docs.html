<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>PufferLib Docs</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <main class="content">
        <nav class="nav-box">
            <ul>
                <li><a href="#post-1">Installation</a></li>
                <li><a href="#post-2">Quickstart Guide</a></li>
                <li><a href="#post-3">Making a New Environment</a></li>
                <li><a href="#post-4">Emulation</a></li>
                <li><a href="#post-5">Vectorization</a></li>
                <li><a href="#post-6">Environments</a></li>
            </ul>
        </nav>

 
        <article id="post-1" class="blog-post">
            <header class="post-header">
                <h1>Installation</h1>
            </header>
            <p>Choose the installation method that best suits your needs.</p>

            <h2>PufferTank (Recommended)</h2>
            <p><a href="https://github.com/pufferai/puffertank" target="_blank">PufferTank</a> is a GPU container with PufferLib and dependencies for all environments in the registry, including some that are slow and tricky to install.</p>
            <div class="info-box">
                <p>If you have not used containers before and just want everything to work, clone the repository and open it in VSCode. You will need to install the Dev Container plugin as well as Docker Desktop. VSCode will then detect the settings in .devcontainer and set up the container for you.</p>
            </div>

            <h2>Pip Installation</h2>
            <p>PufferLib is available as a standard pip package:</p>
            <pre><code>pip install pufferlib</code></pre>
            <p>To install additional environments and frameworks:</p>
            <pre><code>pip install pufferlib[nmmo,cleanrl]</code></pre>

            <h2>Source Installation</h2>
            <pre><code>git clone https://github.com/pufferai/pufferlib
cd pufferlib
pip install -e .</code></pre>
            <p>Add optional dependencies:</p>
            <pre><code>pip install -e .[nmmo,cleanrl]</code></pre>
        </article>


        <article id="post-2" class="blog-post">
            <header class="post-header">
                <h1>Quickstart Guide</h1>
                <p>When you install the PufferLib pip package, you are getting our core tools. The main ones are:</p>
                <ul>
                    <li><strong>Emulation:</strong> A compatibility layer that makes environments faster and easier to work with, in or ourside of PufferLib. This was the initial inspiration for PufferLib. Use it by wrapping your environment in pufferlib.emulation.GymnasiumPufferEnv or PettingZooPufferEnv. Your environment will still be in Gymnasium/PZ format, but it will use a subset of the API that is easier for most libraries to deal with.</li>
                    <li><strong>Vectorization:</strong> Fast parallelization for your environments. We provide pufferlib.vector.Serial/Multiprocessing/Ray, as well as a PufferLib.vector.make function for convenience. Our Multiprocessing implementation can be over 10x faster than Gymnasium's for some environments, and you will almost always at least get a noticeable speedup.</li>
                    <li><strong>Ocean:</strong> Our first-party suite of ultra performant environments written in C. They use our native PufferEnv API and each run 1M+ steps/second per CPU core. Import each env directly or use pufferlib.environments.ocean.env_creator(name).</li>
                </ul>

                <p>That's the core API surface. Excluding the code for individual environments, it's only around 1500 lines total. Emulation is one file, vectorization is another, and the native environment API is a 100 line class. Most of the other code in PufferLib is there to support specific legacy environments that researchers still use, or to improve compatibility with other libraries.</p>

                <p>When you install PufferLib from GitHub, you also get the latest training demos. These are not included in the Pip package because they are based on CleanRL, which is designed to be part of user space. Run the demo.py file with --help to see all the options. This is a responsive interface, so if you specify a specific --env, you will see options for that environment. The demo file is just a wrapper around clean_pufferl, our customized version of CleanRL's PPO + LSTM implementation, which is capable of training at over 1M steps per second. Additionally, we provide a nice local dashboard that renders in your terminal, WandB integration, and the CARBS algorithm for hyperparameter sweeps. See config/env_name.ini hyperparameters. Here's how to run a quick training demo:</p>

                <pre><code>python demo.py --env ocean_breakout --mode train --vec native</code></pre>

                <p>You can save to wandb with --track (add your username to the args in demo.py first) and run a hyperparameter sweep with --mode sweep_carbs. To watch your trained policy, set --mode eval --eval-model-path experiments/exp_name/your_checkpoint.pt.</p>

                <p>For a simpler demo, check out cleanrl_ppo_atari.py, which is copied from CleanRL. Only a few line changes were required to integrate PufferLib's faster Atari binding and vectorization. You can do the same with any algorithm of your choice.</p>
            </header>
        </article>

        <article id="post-3" class="blog-post">
            <header class="post-header">
                <h1>Making a New Environment</h1>
                <p>PufferLib gives you the maximum performance based on the amount of engineering effort you are willing to put into your environment. From fastest to slowest:</p>
                <ul>
                    <li><strong>Native PufferEnv in C:</strong> All env logic in C. Carefully written C++ or any other language with a good binding to Python is fine too, but we find C the easiest. All of our new Ocean envs are written this way. One nice benefit is that C compiles to WASM, so it is easy to run web demos.</li>
                    <li><strong>Native PufferEnv in Cython:</strong> All env logic in Cython. This can be as fast as C but requires careful optimization. We provide this option because it is easy for researchers familiar with Python, and we use Cython as a binding layer for C envs anyways, so it is easy to port later.</li>
                    <li><strong>Native PufferEnv in Python:</strong> Uses our native API but keeps all logic in Python. Because of the way we manage observation memory, this can still be much faster than Gymnasium/PettingZoo</li>
                    <li><strong>Gymnasium/PettingZoo:</strong>: Write a standard pre-pufferlib environment and use our Gymnasium/PettingZoo emulation layer for fast parallel simulation.</li>
                    <li><strong>Not using PufferLib:</strong> A return to the dark ages.</li>
                </ul>

                <p>PufferEnv is a vector format designed to handle multiple copies of your environment. It provides preallocated buffers for observations, rewards, etc. that environments write to directly. This circumvents the need for a lot of slow stacking and concatenation logic normally needed in vector environments. All you have to to is pass a slice of these buffers to your environment and compute observations/rewards etc. directly there. When multiprocessing, PufferLib will allocate these buffers in shared memory, so your observation data will be available on the main process immediately.</p>

                <p>A minimal example is provided in ocean/sample. Use this as a template for your environment.</p>

        </article>

            <article id="post-4" class="blog-post">
                <header class="post-header">
                    <h1>Emulation</h1>
                </header>
                    <p>Complex environments may have heirarchical observations and actions, variable numbers of agents, and other quirks that make them difficult to work with and incompatible with standard reinforcement learning libraries. PufferLib's emulation layer makes every environment look like it has flat observations/actions and a constant number of agents. Here's how it works with NetHack and Neural MMO, two notoriously complex environments.</p>

                    <pre><code>import pufferlib.emulation
import pufferlib.wrappers

import nle, nmmo

def nmmo_creator():
  env = nmmo.Env()
  env = pufferlib.wrappers.PettingZooTruncatedWrapper(env)
  return pufferlib.emulation.PettingZooPufferEnv(env=env)

def nethack_creator():
  return pufferlib.emulation.GymnasiumPufferEnv(env_creator=nle.env.NLE)</code></pre>

                    <p>The wrappers give you back a Gymnasium/PettingZoo compliant environment. There is no loss of generality and no change to the underlying environment. You can wrap environments by class, creator function, or object, with or without additional arguments. These wrappers enable us to make some optimizations to vectorization code that would be difficult to implement otherwise. You can choose from a variety of vectorization backends. They all share the same interface with synchronous and asynchronous options.</p>

                    <pre><code>import pufferlib.vector
backend = pufferlib.vector.Serial #or Multiprocessing, Ray
envs = pufferlib.vector.make(nmmo_creator, backend=backend, num_envs=4)

# Synchronous API - reset/step
obs, infos = envs.reset()

# Asynchronous API - async_reset, send/recv
envs.async_reset()
obs, rewards, terminals, truncateds, infos, env_id, mask = envs.recv()</code></pre>

                    <p>Our backends support asynchronous on-policy sampling through a Python implementation of EnvPool. This makes them *faster* than the implementations that ship with most RL libraries. We suggest Serial for debugging and Multiprocessing for most training runs. Ray is a good option if you need to scale beyond a single machine.</p>

                    <p>PufferLib allows you to write vanilla PyTorch policies and use them with multiple learning libraries. We take care of the details of converting between the different APIs. Here's a policy that will work with *any* environment, with a one-line wrapper for CleanRL.</p>

                    <pre><code>import torch
from torch import nn
import numpy as np

import pufferlib.frameworks.cleanrl

class Policy(nn.Module):
  def __init__(self, env):
      super().__init__()
      self.encoder = nn.Linear(np.prod(
          envs.single_observation_space.shape), 128)
      self.decoders = nn.ModuleList([nn.Linear(128, n)
          for n in envs.single_action_space.nvec])
      self.value_head = nn.Linear(128, 1)

  def forward(self, env_outputs):
      env_outputs = env_outputs.reshape(env_outputs.shape[0], -1)
      hidden = self.encoder(env_outputs)
      actions = [dec(hidden) for dec in self.decoders]
      value = self.value_head(hidden)
      return actions, value

obs = torch.Tensor(obs)
policy = Policy(envs.driver_env)
cleanrl_policy = pufferlib.frameworks.cleanrl.Policy(policy)
actions = cleanrl_policy.get_action_and_value(obs)[0].numpy()
obs, rewards, terminals, truncateds, infos = envs.step(actions)
envs.close()</code></pre>

                    <p>Optionally, you can class break the forward pass into an encode and decode step, which allows us to handle recurrance for you. So far, the code above is fully general and does not rely on PufferLib support for specific environments. For convenience, we also provide environment hooks with standard wrappers and baseline models. Here's a complete example.</p>

                    <pre><code>import torch

import pufferlib.models
import pufferlib.vector
import pufferlib.frameworks.cleanrl
import pufferlib.environments.nmmo

make_env = pufferlib.environments.nmmo.env_creator()
envs = pufferlib.vector.make(make_env, backend=backend, num_envs=4)

policy = pufferlib.environments.nmmo.Policy(envs.driver_env)
cleanrl_policy = pufferlib.frameworks.cleanrl.Policy(policy)

env_outputs = envs.reset()[0]
obs = torch.from_numpy(env_outputs)
actions = cleanrl_policy.get_action_and_value(obs)[0].numpy()
next_obs, rewards, terminals, truncateds, infos = envs.step(actions)
envs.close()</code></pre>

                    <p>It's that simple -- almost. If you have an environment with structured observations, you'll have to unpack them in the network forward pass since PufferLib will flatten them in emulation. We provide a utility for this.</p>

                    <pre><code>dtype = pufferlib.pytorch.nativize_dtype(envs.driver_env.emulated)
env_outputs = pufferlib.pytorch.nativize_tensor(obs, dtype)
print('Packed tensor:', obs.shape)
print('Unpacked:', env_outputs.keys())</code></pre>

                    <p>That's all you need to get started. The PufferLib repository contains full-length CleanRL scripts with PufferLib integration. Single-agent environments should work with SB3, and other integrations will be based on demand - so let us know what you want!</p>
                </article>

            <article id="post-5" class="blog-post">
                <header class="post-header">
                    <h1>Vectorization</h1>
                </header>

                <p>Our Multiprocessing backend is fast -- much faster than Gymnasium's in most cases. Atari runs 50-60% faster synchronous and 5x faster async by our latest benchmark, and some environments like NetHack can be 10x faster even synchronous, with no API changes. PufferLib implements the following optimizations:</p>
                <ul>
                    <li><strong>A Python implementation of EnvPool.</strong> Simulates more envs than are needed per batch and returns batches of observations as soon as they are ready. Requires using the async send/recv instead of the sync step API.</li>
                    <li><strong>Multiple environments per worker.</strong> Important for fast environments.</li>
                    <li><strong>Shared memory.</strong> Unlike Gymnasium's implementation, we use a single buffer that is shared across environments.</li>
                    <li><strong>Shared flags.</strong> Workers busy-wait on an unlocked flag instead of signaling via pipes or queues. This virtually eliminates interprocess communication overhead. Pipes are used once per episode to communicate aggregated infos.</li>
                    <li><strong>Zero-copy batching.</strong> Because we use a single buffer for shared memory, we can return observations from contiguous subsets of workers without ever copying observations. Only does not work for full-async mode.</li>
                    <li><strong>Native multiagent support.</strong> It's not an extra wrapper or slow bolt-on feature. PufferLib treats single-agent and multi-agent environments the same. API differences are handled at the emulation level.</li>
                </ul>

                <p>Most of these optimizations are made possible by a hard assumption on PufferLib emulation. This means that we do not need to handle structured data within the vectorization layer itself.</p>
            </article>

            <article id="post-6" class="blog-post">
                <header class="post-header">
                    <h1>Environments</h1>
                </header>

                <p>PufferLib ships with Ocean, our first-party testing suite, which will let you catch 90% of implementation bugs in a 10 second training run. We also provide integrations for many environments out of the box. Non-pip dependencies are already set up for you in PufferTank. Several environments also include reasonable baseline policies. Join our Discord if you would like to add setup and tests for new environments or improvements to any of the baselines.</p>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/openai/gym" target="_blank">
                        <img src="https://img.shields.io/github/stars/openai/gym?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star OpenAI Gym" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/openai/gym">OpenAI Gym</a> is the standard API for single-agent reinforcement learning environments. It also contains some built-in environments. We include <a href="https://www.gymlibrary.dev/environments/box2d/">Box2D</a> in our registry.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/PWhiddy/PokemonRedExperiments" target="_blank">
                        <img src="https://img.shields.io/github/stars/PWhiddy/PokemonRedExperiments?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Pokemon Red" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/PWhiddy/PokemonRedExperiments">Pokemon Red</a> is one of the original Pokemon games for gameboy. This project uses the game as an environment for reinforcement learning. We are actively supporting development on this one!</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Farama-Foundation/PettingZoo" target="_blank">
                        <img src="https://img.shields.io/github/stars/Farama-Foundation/PettingZoo?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star PettingZoo" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://pettingzoo.farama.org">PettingZoo</a> is the standard API for multi-agent reinforcement learning environments. It also contains some built-in environments. We include <a href="https://pettingzoo.farama.org/environments/butterfly/">Butterfly</a> in our registry.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Farama-Foundation/Arcade-Learning-Environment" target="_blank">
                        <img src="https://img.shields.io/github/stars/Farama-Foundation/Arcade-Learning-Environment?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Arcade Learning Environment" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/Farama-Foundation/Arcade-Learning-Environment">Arcade Learning Environment</a> provides a Gym interface for classic Atari games. This is the most popular benchmark for reinforcement learning algorithms.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Farama-Foundation/Minigrid" target="_blank">
                        <img src="https://img.shields.io/github/stars/Farama-Foundation/Minigrid?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Minigrid" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/Farama-Foundation/Minigrid">Minigrid</a> is a 2D grid-world environment engine and a collection of builtin environments. The target is flexible and computationally efficient RL research.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/geek-ai/MAgent" target="_blank">
                        <img src="https://img.shields.io/github/stars/geek-ai/MAgent?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star MAgent" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/geek-ai/MAgent/blob/master/doc/get_started.md">MAgent</a> is a platform for large-scale agent simulation.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/neuralmmo/environment" target="_blank">
                        <img src="https://img.shields.io/github/stars/openai/neural-mmo?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Neural MMO" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://neuralmmo.github.io">Neural MMO</a> is a massively multiagent environment for reinforcement learning. It combines large agent populations with high per-agent complexity and is the most actively maintained (by me) project on this list.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/openai/procgen" target="_blank">
                        <img src="https://img.shields.io/github/stars/openai/procgen?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Procgen" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/openai/procgen">Procgen</a> is a suite of arcade games for reinforcement learning with procedurally generated levels. It is one of the most computationally efficient environments on this list.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/facebookresearch/nle" target="_blank">
                        <img src="https://img.shields.io/github/stars/facebookresearch/nle?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star NLE" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/facebookresearch/nle">Nethack Learning Environment</a> is a port of the classic game NetHack to the Gym API. It combines extreme complexity with high simulation efficiency.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/facebookresearch/minihack" target="_blank">
                        <img src="https://img.shields.io/github/stars/facebookresearch/minihack?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star MiniHack" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/facebookresearch/nle">MiniHack Learning Environment</a> is a stripped down version of NetHack with support for level editing and custom procedural generation.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/danijar/crafter" target="_blank">
                        <img src="https://img.shields.io/github/stars/danijar/crafter?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Crafter" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/danijar/crafter">Crafter</a> is a top-down 2D Minecraft clone for RL research. It provides pixel observations and relatively long time horizons.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Bam4d/Griddly" target="_blank">
                        <img src="https://img.shields.io/github/stars/Bam4d/Griddly?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star Griddly" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://griddly.readthedocs.io/en/latest/">Griddly</a> is an extremely optimized platform for building reinforcement learning environments. It also includes a large suite of built-in environments.</p>
                </div>
            </div>

            <div style="display: flex; align-items: center; margin-bottom: 15px;">
                <div style="flex-shrink: 0; width: 100px; margin-right: 20px;">
                    <a href="https://github.com/Farama-Foundation/MicroRTS-Py" target="_blank">
                        <img src="https://img.shields.io/github/stars/Farama-Foundation/MicroRTS-Py?labelColor=999999&color=66dcdc&cacheSeconds=100000" alt="Star MicroRTS-Py" width="100px">
                    </a>
                </div>
                <div>
                    <p><a href="https://github.com/Farama-Foundation/MicroRTS-Py">Gym MicroRTS</a> is a real time strategy engine for reinforcement learning research. The Java configuration is a bit finicky -- we're still debugging this.</p>
                </div>
            </div>
            </article>

        </div>
    </main>
    <script src="header.js"></script>
    <link rel="stylesheet" href="highlight.css">
    <script src="highlight.js"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
